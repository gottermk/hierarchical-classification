{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da855c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "data = pd.read_excel(\"data/bold_data.xlsx\")\n",
    "rows = np.array(data[\"image_urls\"])\n",
    "downloaded = os.listdir(\"data/images\")\n",
    "\n",
    "# Download images from bold_data.xlsx\n",
    "for r in rows:\n",
    "    try:\n",
    "        urls = r.split(\"|\")\n",
    "        for url in urls:\n",
    "            file_name = url.split(\"/\")[-1]\n",
    "            \n",
    "            # Skip file if already downloaded\n",
    "            if file_name in downloaded:\n",
    "                continue\n",
    "\n",
    "            urllib.request.urlretrieve(url, f\"data/images/{file_name}\")\n",
    "            print(url)\n",
    "    except Exception as e:\n",
    "        print(url, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e4283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01345e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet101\", pretrained=True)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be4a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "directory = \"data/images\"\n",
    "images = os.listdir(directory)\n",
    "data = pd.read_excel(\"data/bold_data.xlsx\", keep_default_na=\"\")\n",
    "X_data = np.empty((0, 2048))\n",
    "Y_data = np.empty((0, 7))\n",
    "\n",
    "for image in images:\n",
    "    image_id = image[:-4]\n",
    "    print(f\"Processing {image_id}\")\n",
    "\n",
    "    try:\n",
    "        input_image = Image.open(f\"{directory}/{image}\")\n",
    "        preprocess = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        input_tensor = preprocess(input_image)\n",
    "        # create a mini-batch as expected by the model\n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "        # move the input and model to GPU for speed if available\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch = input_batch.to(\"cuda\")\n",
    "            model.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "\n",
    "        # Match image with corresponding labels\n",
    "        record = data.loc[data[\"image_urls\"].str.contains(image_id, regex=False)][0:1]\n",
    "        labels = np.array(\n",
    "            record[\n",
    "                [\n",
    "                    \"processid\",\n",
    "                    \"class_name\",\n",
    "                    \"order_name\",\n",
    "                    \"family_name\",\n",
    "                    \"genus_name\",\n",
    "                    \"species_name\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "        if record.shape[0] == 0:\n",
    "            print(\"Skipping\", image_id)\n",
    "            continue\n",
    "        Y_data = np.vstack((Y_data, np.hstack((np.array([[image_id]]), labels))))\n",
    "        X_data = np.vstack((X_data, output[0].numpy().reshape(2048)))\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error:\", image_id, e)\n",
    "\n",
    "# Save X_data and Y_data\n",
    "np.savetxt(f\"data/X_data.txt\", X_data, delimiter=\"\\t\", fmt=\"%s\")\n",
    "header = \"\\t\".join([\"id\", \"processid\", \"class\", \"order\", \"family\", \"genus\", \"species\"])\n",
    "np.savetxt(\n",
    "    f\"data/Y_data.txt\", Y_data, header=header, comments=\"\", delimiter=\"\\t\", fmt=\"%s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975df19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba67a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import dump\n",
    "\n",
    "X = pd.read_csv(\"data/X_data.txt\", delimiter=\"\\t\", header=None)\n",
    "\n",
    "columns = [\"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "Y = np.array(pd.read_csv(\"data/Y_data.txt\", delimiter=\"\\t\")[columns])\n",
    "\n",
    "# 80/20 split for training/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize to mean of 0 and standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "dump(scaler, f\"scaler.joblib\")\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reduce to 500 dimensions\n",
    "pca = PCA(n_components=500)\n",
    "pca.fit(X_train_scaled)\n",
    "dump(pca, f\"pca.joblib\")\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Save training and test data\n",
    "np.savetxt(f\"data/X_train_pca.txt\", X_train_pca, delimiter=\"\\t\")\n",
    "np.savetxt(f\"data/X_test_pca.txt\", X_test_pca, delimiter=\"\\t\")\n",
    "np.savetxt(f\"data/Y_train.txt\", Y_train, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(f\"data/Y_test.txt\", Y_test, delimiter=\"\\t\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffc062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac1d4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da400be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b33d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
